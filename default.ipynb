{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/openclass-192.jpeg\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - Summary -\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warehouse automation with robots is crucial in the present for enhancing operational efficiency, reducing labor costs, and ensuring accuracy in inventory management. In the future, it will be even more vital as demand for faster fulfillment and scalability increases, driven by the growth of e-commerce and the need for resilient supply chains.\n",
    "\n",
    "In this Open Class, you will learn how to **combine Perception and Grasping techniques** to explore warehouse automation with ROS 2.\n",
    "\n",
    "What you'll learn:\n",
    "- Combining **Perception** and **Grasping** for Automation: Develop skills to seamlessly integrate perception and grasping techniques to create advanced robotic solutions that can autonomously detect, pick, and place objects in a warehouse environment.\n",
    " \n",
    "You'll be using the **BOTBOX** throughout the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOTBOX is a lab-in-a-box to teach robotics, including off-the-shelf robots, the environment, simulations, and projects for your students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your students need to install nothing in order to start programming the robots. Everything is web based and works in any computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have full control of your studentâ€™s progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/IMG_3494.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/demosim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Botbox package includes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/botbox_package.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get more info at https://www.theconstruct.ai/botbox-warehouse-lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - End of Summary -\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"\">Launch the simulation</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch the project simulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open a terminal by clicking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rosject_toolbar_terminal.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Copy following command in terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 launch tortoisebot_bringup simulation.launch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait around 30 seconds** for simulation to start. It should automatically appear in a Gazebo window.\n",
    "\n",
    "If it doesn't automatically appear, open the Gazebo window by clicking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rosject_toolbar_gazebo.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gazebo window should show the Tortoisebot Warehouse world:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sim_gripper.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - Notes -\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The terminal where the simulation was launched is occupied, it cannot be used unless you simulation is killed. Open new terminals to run other processes.\n",
    "* The simulation or the real robot is needed. If none of these are running, then there will be no incoming data and ROS 2 nodes will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - End Notes -\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Important Information</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Grasping System</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed the simplified gripper in the simulation. In the real robot, make sure that the gripper is also attached:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/humblegrip.jpg\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "If the griper is not attached, use the provided screw, washers and nut to attach the gripper bracket to the bottom of the robot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/humblegrip_screw.jpg\" width=\"275\" align=\"left\"/>\n",
    "<img src=\"images/humblegrip_bottom.jpg\" width=\"275\" align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operate Gripper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **close** gripper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 action send_goal /gripper_controller/gripper_cmd control_msgs/action/GripperCommand \"command:\n",
    "  position: -0.02\n",
    "  max_effort: 0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **open** gripper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 action send_goal /gripper_controller/gripper_cmd control_msgs/action/GripperCommand \"command:\n",
    "  position: 0.02\n",
    "  max_effort: 0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice the `action` prefix to the command, this will come in important later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Today's Mission</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Transport the goods ðŸšš</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice our warehouse looks a little different from last time. This is because the factory has been busy producing goods, and the factory floor has gone through a bit of an upgrade.Â \n",
    "\n",
    "<img src=\"images/map.png\" style=\"width: 400px\" />\n",
    "\n",
    "We will be starting at the same place as the previous classes, which will be within the process station. This is where the factory produces the product which in this case is these cubes with interesting markers on them (if you did the previous few open classes, you will already know what these are).Â \n",
    "\n",
    "<img src=\"images/map_zoomed.png\" style=\"width: 200px\" />\n",
    "\n",
    "For this class, your mission is fairly simple: pick up our product from the processing station and return to the factory line.Â "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Task 1</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Pallet cube perception</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all of our previous lessons, let's first take a look at our product and see what we have to work with.\n",
    "\n",
    "Let's create a new package for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws/src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 pkg create --build-type ament_python pick_and_place --dependencies rclpy std_msgs geometry_msgs cv_bridge sensor_msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our script within the package to run.Â "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/file_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    pick_and_place.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "\n",
    "class ImageSubscriber(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('pick_and_place')\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image,\n",
    "            '/camera/image_raw',\n",
    "            self.listener_callback,\n",
    "            10)\n",
    "        self.subscription  # prevent unused variable warning\n",
    "\n",
    "        self.bridge = CvBridge()\n",
    "\n",
    "    def listener_callback(self, msg):\n",
    "        self.get_logger().info('Receiving video frame')\n",
    "        # Convert ROS Image message to OpenCV image\n",
    "        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n",
    "        \n",
    "        # Display image\n",
    "        cv2.imshow(\"Camera Feed\", cv_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    image_subscriber = ImageSubscriber()\n",
    "\n",
    "    try:\n",
    "        rclpy.spin(image_subscriber)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    # Destroy the node explicitly\n",
    "    image_subscriber.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "    # Close any OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will need to modify the `setup.py` file in order for ROS to see our new file as a node it can run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    setup.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup\n",
    "\n",
    "package_name = 'pick_and_place'\n",
    "\n",
    "setup(\n",
    "    name=package_name,\n",
    "    version='0.0.0',\n",
    "    packages=[package_name],\n",
    "    data_files=[\n",
    "        ('share/ament_index/resource_index/packages',\n",
    "            ['resource/' + package_name]),\n",
    "        ('share/' + package_name, ['package.xml']),\n",
    "    ],\n",
    "    install_requires=['setuptools'],\n",
    "    zip_safe=True,\n",
    "    maintainer='user',\n",
    "    maintainer_email='user@todo.todo',\n",
    "    description='TODO: Package description',\n",
    "    license='TODO: License declaration',\n",
    "    tests_require=['pytest'],\n",
    "    entry_points={\n",
    "        'console_scripts': [\n",
    "            'pick_and_place = pick_and_place.pick_and_place:main'\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build and run our new script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws\n",
    "\n",
    "# Build the packages in the ros workspace\n",
    "colcon build \n",
    "\n",
    "# Source our setup varibles so ros knows where all the built files are\n",
    "source install/setup.bash\n",
    "\n",
    "# Run our new node\n",
    "ros2 run pick_and_place pick_and_place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct you should see the following open up in graphical tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/task1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a brief glimpse of one of the pallet cubes.Â \n",
    "\n",
    "If you look a little closely ... what's that?Â \n",
    "\n",
    "For the people that attended \"Perception with AprilTags for ROS2\" may already know what this is.\n",
    "\n",
    "These are Aruco tags.\n",
    "\n",
    "<img src=\"https://docs.opencv.org/4.x/markers.jpg\" style=\"width: 200px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArUco markers areÂ 2D binary-encoded fiducial patterns designed to be quickly located by computer vision systems. By performing a set of calculations using the patterns detected by the camera, compute the 3D pose of the tag relative to the position of the robot.\n",
    "\n",
    "As you'd imagine, it's extremely important for us to know the position of the tag precisely, as we need to align ourselves with the pallet for our gripper to be able to grab it.Â \n",
    "\n",
    "Now for this class, we will be using the package `ros2_aruco` to get the detected tag id and it's pose.Â \n",
    "\n",
    "* `ros2_aruco` [public repo](https://github.com/JMU-ROBOTICS-VIVA/ros2_aruco) from James Madison University Robotics. This was also used to generate the aruco markers in the WarehousePlanet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 run ros2_aruco aruco_node marker_size:=0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 run teleop_twist_keyboard teleop_twist_keyboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a pallet cube is in view of the robot camera, then you should see a published topic `/aruco_markers`. This contains an array of all marker poses (with respect to robot camera) and its corresponding marker ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int64[] marker_ids\n",
    "geometry_msgs/Pose[] poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to run the entire program in one command and not split across terminals we can make a single launch files that runs both nodes from our own package and the `ros2_aruco` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    launch/pick_and_place.launch.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from launch import LaunchDescription\n",
    "from launch_ros.actions import Node\n",
    "\n",
    "def generate_launch_description():\n",
    "    return LaunchDescription([\n",
    "        Node(\n",
    "            package='pick_and_place',\n",
    "            executable='pick_and_place',\n",
    "            name='pick_and_place_node',\n",
    "            output='screen',\n",
    "        ),\n",
    "        Node(\n",
    "            package='ros2_aruco',\n",
    "            executable='aruco_node',\n",
    "            name='aruco_node',\n",
    "            output='screen',\n",
    "            parameters=[{'marker_size': 0.05}]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate_launch_description()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    setup.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup\n",
    "from glob import glob \n",
    "import os\n",
    "\n",
    "package_name = 'pick_and_place'\n",
    "\n",
    "setup(\n",
    "    name=package_name,\n",
    "    version='0.0.0',\n",
    "    packages=[package_name],\n",
    "    data_files=[\n",
    "        ('share/ament_index/resource_index/packages',\n",
    "            ['resource/' + package_name]),\n",
    "        ('share/' + package_name, ['package.xml']),\n",
    "        (os.path.join('share', package_name), glob('launch/*.launch.py'))\n",
    "    ],\n",
    "    install_requires=['setuptools'],\n",
    "    zip_safe=True,\n",
    "    maintainer='user',\n",
    "    maintainer_email='user@todo.todo',\n",
    "    description='TODO: Package description',\n",
    "    license='TODO: License declaration',\n",
    "    tests_require=['pytest'],\n",
    "    entry_points={\n",
    "        'console_scripts': [\n",
    "            'pick_and_place = pick_and_place.pick_and_place:main'\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build and run our new script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws\n",
    "\n",
    "# Build the packages in the ros workspace\n",
    "colcon build \n",
    "\n",
    "# Source our setup varibles so ros knows where all the built files are\n",
    "source install/setup.bash\n",
    "\n",
    "# Run our new node\n",
    "ros2 launch pick_and_place pick_and_place.launch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll be able to see that once the tag enters the camera frame it gets detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/task-1-1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can see we are getting the information from the tag lets display it on our image view in our script.\n",
    "\n",
    "For that we will need to subscribe to the `aruco_markers` topic that will have all of the information we need and then display it using opencv. \n",
    "\n",
    "Lets decide on what we are going to do.\n",
    "\n",
    "1. Subscribe to the `aruco_markers` topic.\n",
    "2. Add a callback that will take the pose information that we need and store them into the class. \n",
    "3. Using opencv display the tag id, the position using the display text function \n",
    "4. Draw a circle around the center of the detected tag\n",
    "5. Draw a line from our current position to the center of the tag \n",
    "6. Determine the angle from our current position to the tag using the orientation information from the tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `aruco_markers` topic publishes a custom message we will have to import that message definition first.  \n",
    "\n",
    "```python\n",
    "from ros2_aruco_interfaces.msg import ArucoMarkers\n",
    "```\n",
    "\n",
    "Now we can add our subscriber \n",
    "\n",
    "```python\n",
    "        self.marker_subscription = self.create_subscription(\n",
    "            ArucoMarkers,\n",
    "            '/aruco_markers',\n",
    "            self.marker_callback,\n",
    "            10)\n",
    "```\n",
    "\n",
    "And have a variable that stores markers detected\n",
    "\n",
    "```python\n",
    "        self.markers = []\n",
    "```\n",
    "\n",
    "Youâ€™ll notice that we defined a callback when we added our subscriber\n",
    "\n",
    "```python\n",
    "    def marker_callback(self, msg):\n",
    "        self.get_logger().info('Receiving marker information')\n",
    "        self.markers = []\n",
    "        for i, marker_id in enumerate(msg.marker_ids):\n",
    "            position = msg.poses[i].position\n",
    "            orientation = msg.poses[i].orientation\n",
    "            self.markers.append({\n",
    "                'id': marker_id,\n",
    "                'position': {\n",
    "                    'x': position.x,\n",
    "                    'y': position.y,\n",
    "                    'z': position.z\n",
    "                },\n",
    "                'orientation': {\n",
    "                    'x': orientation.x,\n",
    "                    'y': orientation.y,\n",
    "                    'z': orientation.z,\n",
    "                    'w': orientation.w\n",
    "                }\n",
    "            })\n",
    "```\n",
    "\n",
    "This callback stores information about the position and orientation of the detected pose to the markers array. \n",
    "\n",
    "Now that we have the markers array being populated lets display it in our opencv frame.\n",
    "\n",
    "```python\n",
    "        # Center bottom of the frame\n",
    "        center_bottom = (cv_image.shape[1] // 2, cv_image.shape[0])\n",
    "\n",
    "        for marker in self.markers:\n",
    "            marker_id = marker['id']\n",
    "            position = marker['position']\n",
    "            orientation = marker['orientation']\n",
    "            \n",
    "            # Convert position to pixel coordinates\n",
    "            pixel_x = int(position['x'] * 1000 + cv_image.shape[1] / 2)\n",
    "            pixel_y = int(position['y'] * 1000 + cv_image.shape[0] / 2)\n",
    "\n",
    "            # Draw circle at marker position\n",
    "            cv2.circle(cv_image, (pixel_x, pixel_y), 10, (0, 0, 255), -1)\n",
    "\n",
    "            # Draw line from center bottom to marker position\n",
    "            cv2.line(cv_image, center_bottom, (pixel_x, pixel_y), (0, 255, 0), 2)\n",
    "\n",
    "            # Calculate the yaw angle in degrees from quaternion orientation\n",
    "            quaternion = (orientation['x'], orientation['y'], orientation['z'], orientation['w'])\n",
    "            euler = tf_transformations.euler_from_quaternion(quaternion)\n",
    "            yaw_deg = np.degrees(euler[2]) * 100  # Extract yaw (z-axis rotation) and convert to degrees\n",
    "\n",
    "            # Display position and yaw angle\n",
    "            position_text = f\"ID: {marker_id} Pos: ({position['x']:.2f}, {position['y']:.2f}, {position['z']:.2f})\"\n",
    "            yaw_text = f\"Yaw: {yaw_deg:.2f} degrees\"\n",
    "            cv2.putText(cv_image, position_text, (10, 50 + 20 * marker_id), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.putText(cv_image, yaw_text, (10, 70 + 20 * marker_id), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "```\n",
    "\n",
    "In this for each and every marker that is detected which in this case is going to be only one marker we are going to calculated the pixel coordinates of the marker position and then draw a circle so we can see where the tag is being detected as.\n",
    "\n",
    "Then we draw a line from the center bottom to the marker center, this will help us see our position relative to the marker position.\n",
    "\n",
    "After that we calculate the angle difference using the orientation information by first changing the quaternion to euler coordinates and then to degrees. \n",
    "\n",
    "And finally we add the text with the ID, Pos and Yaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    pick_and_place.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image\n",
    "from ros2_aruco_interfaces.msg import ArucoMarkers\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tf_transformations\n",
    "\n",
    "class ImageSubscriber(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('pick_and_place')\n",
    "        self.image_subscription = self.create_subscription(\n",
    "            Image,\n",
    "            '/camera/image_raw',\n",
    "            self.image_callback,\n",
    "            10)\n",
    "        self.marker_subscription = self.create_subscription(\n",
    "            ArucoMarkers,\n",
    "            '/aruco_markers',\n",
    "            self.marker_callback,\n",
    "            10)\n",
    "        \n",
    "        self.bridge = CvBridge()\n",
    "        self.markers = []\n",
    "\n",
    "    def image_callback(self, msg):\n",
    "        self.get_logger().info('Receiving video frame')\n",
    "        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n",
    "\n",
    "        # Center bottom of the frame\n",
    "        center_bottom = (cv_image.shape[1] // 2, cv_image.shape[0])\n",
    "\n",
    "        for marker in self.markers:\n",
    "            marker_id = marker['id']\n",
    "            position = marker['position']\n",
    "            orientation = marker['orientation']\n",
    "            \n",
    "            # Convert position to pixel coordinates\n",
    "            pixel_x = int(position['x'] * 1000 + cv_image.shape[1] / 2)\n",
    "            pixel_y = int(position['y'] * 1000 + cv_image.shape[0] / 2)\n",
    "\n",
    "            # Draw circle at marker position\n",
    "            cv2.circle(cv_image, (pixel_x, pixel_y), 10, (0, 0, 255), -1)\n",
    "\n",
    "            # Draw line from center bottom to marker position\n",
    "            cv2.line(cv_image, center_bottom, (pixel_x, pixel_y), (0, 255, 0), 2)\n",
    "\n",
    "            # Calculate the yaw angle in degrees from quaternion orientation\n",
    "            quaternion = (orientation['x'], orientation['y'], orientation['z'], orientation['w'])\n",
    "            euler = tf_transformations.euler_from_quaternion(quaternion)\n",
    "            yaw_deg = np.degrees(euler[2]) * 100  # Extract yaw (z-axis rotation) and convert to degrees\n",
    "\n",
    "            # Display position and yaw angle\n",
    "            position_text = f\"ID: {marker_id} Pos: ({position['x']:.2f}, {position['y']:.2f}, {position['z']:.2f})\"\n",
    "            yaw_text = f\"Yaw: {yaw_deg:.2f} degrees\"\n",
    "            cv2.putText(cv_image, position_text, (10, 50 + 20 * marker_id), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.putText(cv_image, yaw_text, (10, 70 + 20 * marker_id), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Camera Feed\", cv_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    def marker_callback(self, msg):\n",
    "        self.get_logger().info('Receiving marker information')\n",
    "        self.markers = []\n",
    "        for i, marker_id in enumerate(msg.marker_ids):\n",
    "            position = msg.poses[i].position\n",
    "            orientation = msg.poses[i].orientation\n",
    "            self.markers.append({\n",
    "                'id': marker_id,\n",
    "                'position': {\n",
    "                    'x': position.x,\n",
    "                    'y': position.y,\n",
    "                    'z': position.z\n",
    "                },\n",
    "                'orientation': {\n",
    "                    'x': orientation.x,\n",
    "                    'y': orientation.y,\n",
    "                    'z': orientation.z,\n",
    "                    'w': orientation.w\n",
    "                }\n",
    "            })\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    image_subscriber = ImageSubscriber()\n",
    "\n",
    "    try:\n",
    "        rclpy.spin(image_subscriber)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    image_subscriber.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/task-1-2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">2</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Pallet cube grasping</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to see the cube we need to be able to grab the cube so that we can take it wherever we need to take it. \n",
    "\n",
    "For that we need to do a couple of steps.\n",
    "\n",
    "1. Setup `cmd_vel` publisher. \n",
    "2. Turn until the tag is visible.\n",
    "3. Open our gripper.\n",
    "4. Approach the tag until we are sure that we have the cube inside of our gripper.\n",
    "5. Close the gripper and stop the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The turning is fairly simple so lets about the gripper for a bit.\n",
    "\n",
    "Youâ€™ll have noticed the command for the gripper is a bit different that what you might be used to. \n",
    "\n",
    "```\n",
    "ros2 action send_goal /gripper_controller/gripper_cmd control_msgs/action/GripperCommand \"command: position: -0.02\\n  max_effort: 0.0\"\n",
    "```\n",
    "\n",
    "This is because `/gripper_controller/gripper_cmd` is actually an action server.\n",
    "\n",
    "![](https://docs.ros.org/en/foxy/_images/Action-SingleActionClient.gif)\n",
    "\n",
    "Now donâ€™t worry we wonâ€™t be going much into action servers in this class but if your curious you can watch this video. \n",
    "\n",
    "https://www.youtube.com/watch?v=ICpsNT3lhaU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first add our new publisher to `cmd_vel` and our action client for `/gripper_controller/gripper_cmd`\n",
    "\n",
    "```python\n",
    "    self.cmd_vel_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n",
    "    self.gripper_action_client = ActionClient(self, GripperCommand, '/gripper_controller/gripper_cmd')\n",
    "```\n",
    "\n",
    "We will also add a new flags to track our gripper state.\n",
    "\n",
    "```python\n",
    "    self.gripper_opened = False\n",
    "    self.gripper_closed = False\n",
    "```\n",
    "\n",
    "For our first step which is to turn left its quite simple, we just need to have a check that if there are no detected tags we turn left. \n",
    "\n",
    "```python\n",
    "    twist = Twist()\n",
    "\n",
    "    if not self.markers:\n",
    "        # If no markers are detected, turn left\n",
    "        twist.angular.z = 0.5  # Turn left at a constant rate\n",
    "    else:\n",
    "\t\t    ...\n",
    "\t\t    \n",
    "        for marker in self.markers:\n",
    "            marker_id = marker['id']\n",
    "            position = marker['position']\n",
    "            orientation = marker['orientation']\n",
    "\n",
    "\t\t\t\t\t\t...\n",
    "\n",
    "     # Publish the accumulated twist command\n",
    "     self.cmd_vel_publisher.publish(twist)\n",
    "```\n",
    "\n",
    "Now once we have detected the tag we need to open our gripper. \n",
    "\n",
    "For that we will use,\n",
    "\n",
    "```python\n",
    "        # If marker detected\n",
    "        if not self.gripper_opened:\n",
    "            # Open the gripper\n",
    "            self.send_gripper_command(0.02, 0.0)\n",
    "            self.gripper_opened = True\n",
    "```\n",
    "\n",
    "This checks if our gripper is opened when its supposed to be the if its not we will use the `send_gripper_command` to open the gripper.\n",
    "\n",
    "`send_gripper_command` is a new function that we will write that makes use of that action server client. This does essentially the asynchronous version of the command you ran earlier. \n",
    "\n",
    "```python\n",
    "def send_gripper_command(self, position, max_effort):\n",
    "    goal_msg = GripperCommand.Goal()\n",
    "    goal_msg.command.position = position\n",
    "    goal_msg.command.max_effort = max_effort\n",
    "\n",
    "    self.gripper_action_client.wait_for_server()\n",
    "    self.gripper_action_client.send_goal_async(goal_msg)\n",
    "```\n",
    "\n",
    "Ok now we have the gripper open and our cube in sight.\n",
    "\n",
    "How are we supposed to approach the cube and make sure we are approaching it straight. For this we will be applying a simple proportional algorithm based off the offset of the robot from the center of the tag.\n",
    "\n",
    "$C_z = error_x \\times K_p$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $C_z$ is the control output\n",
    "- $error_x$ is the difference between the current position and the desired position (center of the tag)\n",
    "- $K_p$ is the proportional gain\n",
    "\n",
    "```python\n",
    "                # Calculate the horizontal offset from the center\n",
    "                offset = pixel_x - center_bottom[0]\n",
    "\n",
    "                # Align with the tag using proportional control\n",
    "                k_p = 0.007  # Proportional control constant\n",
    "                twist.angular.z = -k_p * offset  # Proportional control for alignment\n",
    "\n",
    "                # Move forward slowly\n",
    "                twist.linear.x = 0.02\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image\n",
    "from ros2_aruco_interfaces.msg import ArucoMarkers\n",
    "from geometry_msgs.msg import Twist\n",
    "from cv_bridge import CvBridge\n",
    "from rclpy.action import ActionClient\n",
    "from control_msgs.action import GripperCommand\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tf_transformations\n",
    "\n",
    "class ImageSubscriber(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('pick_and_place')\n",
    "        self.image_subscription = self.create_subscription(\n",
    "            Image,\n",
    "            '/camera/image_raw',\n",
    "            self.image_callback,\n",
    "            10)\n",
    "        self.marker_subscription = self.create_subscription(\n",
    "            ArucoMarkers,\n",
    "            '/aruco_markers',\n",
    "            self.marker_callback,\n",
    "            10)\n",
    "        \n",
    "        self.cmd_vel_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n",
    "        self.gripper_action_client = ActionClient(self, GripperCommand, '/gripper_controller/gripper_cmd')\n",
    "        \n",
    "        self.bridge = CvBridge()\n",
    "        self.markers = []\n",
    "        self.gripper_opened = False\n",
    "        self.gripper_closed = False\n",
    "\n",
    "    def send_gripper_command(self, position, max_effort):\n",
    "        goal_msg = GripperCommand.Goal()\n",
    "        goal_msg.command.position = position\n",
    "        goal_msg.command.max_effort = max_effort\n",
    "\n",
    "        self.gripper_action_client.wait_for_server()\n",
    "        self.gripper_action_client.send_goal_async(goal_msg)\n",
    "\n",
    "    def image_callback(self, msg):\n",
    "        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n",
    "\n",
    "        # Center bottom of the frame\n",
    "        center_bottom = (cv_image.shape[1] // 2, cv_image.shape[0])\n",
    "        twist = Twist()\n",
    "\n",
    "        if not self.markers:\n",
    "            if not self.gripper_closed:\n",
    "                # If no markers are detected, turn left\n",
    "                twist.angular.z = 0.5  # Turn left at a constant rate\n",
    "            else:\n",
    "                twist.linear.x = 0.0\n",
    "                twist.angular.z = 0.0\n",
    "        else:\n",
    "            # If marker detected\n",
    "            if not self.gripper_opened:\n",
    "                # Open the gripper\n",
    "                self.send_gripper_command(0.02, 0.0)\n",
    "                self.gripper_opened = True\n",
    "\n",
    "            for marker in self.markers:\n",
    "                marker_id = marker['id']\n",
    "                position = marker['position']\n",
    "                orientation = marker['orientation']\n",
    "                \n",
    "                # Convert position to pixel coordinates\n",
    "                pixel_x = int(position['x'] * 1000 + cv_image.shape[1] / 2)\n",
    "                pixel_y = int(position['y'] * 1000 + cv_image.shape[0] / 2)\n",
    "\n",
    "                # Draw circle at marker position\n",
    "                cv2.circle(cv_image, (pixel_x, pixel_y), 10, (0, 0, 255), -1)\n",
    "\n",
    "                # Draw line from center bottom to marker position\n",
    "                cv2.line(cv_image, center_bottom, (pixel_x, pixel_y), (0, 255, 0), 2)\n",
    "\n",
    "                # Calculate the yaw angle in degrees from quaternion orientation\n",
    "                quaternion = (orientation['x'], orientation['y'], orientation['z'], orientation['w'])\n",
    "                euler = tf_transformations.euler_from_quaternion(quaternion)\n",
    "                yaw_deg = np.degrees(euler[2]) * 100  # Extract yaw (z-axis rotation) and convert to degrees\n",
    "\n",
    "                # Display position and yaw angle\n",
    "                position_text = f\"ID: {marker_id} Pos: ({position['x']:.2f}, {position['y']:.2f}, {position['z']:.2f})\"\n",
    "                yaw_text = f\"Yaw: {yaw_deg:.2f} degrees\"\n",
    "                cv2.putText(cv_image, position_text, (10, 50 + 20 * marker_id), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                cv2.putText(cv_image, yaw_text, (10, 70 + 20 * marker_id), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                # Calculate the horizontal offset from the center\n",
    "                offset = pixel_x - center_bottom[0]\n",
    "\n",
    "                # Align with the tag using proportional control\n",
    "                k_p = 0.001  # Proportional control constant\n",
    "                twist.angular.z = -k_p * offset  # Proportional control for alignment\n",
    "\n",
    "                # Move forward slowly\n",
    "                twist.linear.x = 0.02\n",
    "\n",
    "                # Check if the marker is no longer visible (simulate the end of the task)\n",
    "                if not self.markers:\n",
    "                    twist.linear.x = 0.0\n",
    "                    twist.angular.z = 0.0\n",
    "                    if not self.gripper_closed:\n",
    "                        self.send_gripper_command(-0.02, 0.0)\n",
    "                        self.gripper_closed = True\n",
    "\n",
    "        # Publish the accumulated twist command\n",
    "        self.cmd_vel_publisher.publish(twist)\n",
    "\n",
    "        cv2.imshow(\"Camera Feed\", cv_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    def marker_callback(self, msg):\n",
    "        self.markers = []\n",
    "        for i, marker_id in enumerate(msg.marker_ids):\n",
    "            position = msg.poses[i].position\n",
    "            orientation = msg.poses[i].orientation\n",
    "            self.markers.append({\n",
    "                'id': marker_id,\n",
    "                'position': {\n",
    "                    'x': position.x,\n",
    "                    'y': position.y,\n",
    "                    'z': position.z\n",
    "                },\n",
    "                'orientation': {\n",
    "                    'x': orientation.x,\n",
    "                    'y': orientation.y,\n",
    "                    'z': orientation.z,\n",
    "                    'w': orientation.w\n",
    "                }\n",
    "            })\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    image_subscriber = ImageSubscriber()\n",
    "\n",
    "    try:\n",
    "        rclpy.spin(image_subscriber)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    image_subscriber.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/final-1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/humblegrip_grasp.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Challenge</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Autonomous Return and Release</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've successfully grabbed the cube, your next challenge is to navigate back to the centerline and then release the gripper.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "1. Program your robot to reverse its path and return to the centerline.\n",
    "2. Once the robot has reached the centerline, program it to release the gripper and drop the cube.\n",
    "\n",
    "Remember to use make sure of the gripper functions that weâ€™ve used thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Homework</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Combine !!!</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that you've understood how to:\n",
    "\n",
    "1. Detect and track a cube using OpenCV and ArUco markers,\n",
    "2. Open and close the gripper, and\n",
    "3. Navigate using the line following skills\n",
    "\n",
    "You'll have noticed a single cube in the storage bay. Your task, should you choose to accept it, is to use the line following skills you have learned in the previous open class to navigate to the storage bay, and using the information that you have learned in this open class, grab the cube. Once you have the cube, navigate to the dispatch area and release the cube."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
